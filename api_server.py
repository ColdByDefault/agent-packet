"""
FastAPI server for the Local LLM Agent.
Provides REST API endpoints to interact with the agent.
"""

import asyncio
import sys
import threading
from pathlib import Path
from typing import Dict, Any
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn

# Add src to path to import the agent
sys.path.insert(0, str(Path(__file__).parent / "src"))

from llm_agent import LocalLLMAgent, AgentConfig
from llm_agent.core.config import OllamaConfig, RAGConfig, MCPConfig

# Global agent instance
agent: LocalLLMAgent | None = None
restart_flag = False


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Lifespan event handler for startup and shutdown."""
    global agent
    
    # Startup - Silent initialization
    config = AgentConfig(
        agent_name="LocalAgent",
        system_prompt="""You are a helpful AI assistant running locally. You have access to:
        - Your local knowledge base for retrieving relevant information
        - Various tools for calculations, text search, and system information
        - The ability to remember our conversation even after restarts
        
        Always be helpful, accurate, and mention when you're using your knowledge base or tools.""",
        
        ollama=OllamaConfig(
            base_url="http://localhost:11434",
            model="llama3.1:8b",
            temperature=0.7
        ),
        
        rag=RAGConfig(
            vector_db_path="./data/vectordb",
            embedding_model="nomic-embed-text:latest",
            chunk_size=800,
            max_results=5
        ),
        
        mcp=MCPConfig(
            enabled=True,  # Set to False to disable MCP and run REST API only
            server_port=8002  # MCP internal server on different port
        )
    )
    
    # Create and initialize agent
    agent = LocalLLMAgent(config)
    
    try:
        await agent.initialize()
    except Exception as e:
        print(f"‚ö†Ô∏è  Warning: Could not fully initialize agent: {e}")
    
    yield
    
    # Shutdown
    if agent:
        await agent.cleanup()


# Create FastAPI app with lifespan
app = FastAPI(
    title="Local LLM Agent API",
    description="REST API for interacting with the Local LLM Agent",
    version="1.0.0",
    lifespan=lifespan
)

# Configure CORS to allow frontend connections
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://localhost:3001"],  # Next.js dev server
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global agent instance (removed as it's now in lifespan)
restart_flag = False


def listen_for_restart():
    """Listen for 'rs' command to restart the server."""
    global restart_flag
    
    while not restart_flag:
        try:
            user_input = input().strip().lower()
            if user_input == "rs":
                restart_flag = True
                import os
                os._exit(0)
        except (EOFError, KeyboardInterrupt):
            break


class ChatRequest(BaseModel):
    """Request model for chat endpoint."""
    message: str


class ChatResponse(BaseModel):
    """Response model for chat endpoint."""
    response: str
    metadata: Dict[str, Any] = {}


@app.get("/")
async def root():
    """Root endpoint."""
    return {
        "message": "Local LLM Agent API",
        "status": "running",
        "version": "1.0.0"
    }


@app.get("/health")
async def health_check():
    """Health check endpoint to verify backend is running."""
    mcp_tools = None
    if agent and agent._initialized and agent.config.mcp.enabled and agent.mcp_server:
        try:
            # Get available tools from MCP server (synchronous call)
            mcp_tools = agent.mcp_server.tool_registry.get_available_tools()
        except Exception as e:
            print(f"Error getting tools: {e}")
    
    return {
        "status": "healthy",
        "agent_initialized": agent is not None and agent._initialized,
        "agent_name": agent.config.agent_name if agent else None,
        "mcp_enabled": agent.config.mcp.enabled if agent else None,
        "mcp_tools": mcp_tools,
        "timestamp": asyncio.get_event_loop().time()
    }


@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    Chat with the agent.
    
    Args:
        request: Chat request with user message
        
    Returns:
        Agent's response
    """
    if not agent:
        raise HTTPException(status_code=503, detail="Agent not initialized")
    
    if not agent._initialized:
        raise HTTPException(status_code=503, detail="Agent still initializing")
    
    try:
        # Chat with the agent
        response_text = await agent.chat(request.message)
        
        return ChatResponse(
            response=response_text,
            metadata={
                "model": agent.config.ollama.model,
            }
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error processing chat: {str(e)}")


@app.get("/status")
async def get_status():
    """Get detailed agent status."""
    if not agent:
        return {"status": "not_initialized"}
    
    # Get MCP tools if available
    mcp_tools = None
    if agent.config.mcp.enabled and agent.mcp_server:
        try:
            mcp_tools = agent.mcp_server.tool_registry.get_available_tools()
        except Exception:
            mcp_tools = None
    
    return {
        "status": "ready" if agent._initialized else "initializing",
        "agent_name": agent.config.agent_name,
        "llm_model": agent.config.ollama.model,
        "ollama_url": agent.config.ollama.base_url,
        "rag_enabled": agent.rag_system is not None,
        "mcp_enabled": agent.config.mcp.enabled,
        "mcp_tools": mcp_tools,
        "conversation_length": len(agent.conversation.messages)
    }


@app.get("/conversation")
async def get_conversation():
    """Get the current conversation history."""
    if not agent:
        raise HTTPException(status_code=503, detail="Agent not initialized")
    
    if not agent._initialized:
        raise HTTPException(status_code=503, detail="Agent still initializing")
    
    # Get messages and convert to serializable format
    messages = []
    for msg in agent.conversation.messages:
        messages.append({
            "role": msg.role.value,  # Convert enum to string
            "content": msg.content,
            "timestamp": None,  # Backend doesn't store timestamps
        })
    
    return {
        "messages": messages,
        "conversation_length": len(messages)
    }


@app.delete("/conversation")
async def clear_conversation():
    """Clear the conversation history."""
    if not agent:
        raise HTTPException(status_code=503, detail="Agent not initialized")
    
    if not agent._initialized:
        raise HTTPException(status_code=503, detail="Agent still initializing")
    
    agent.conversation.clear()
    
    return {
        "message": "Conversation cleared",
        "conversation_length": 0
    }


@app.post("/conversation/new")
async def start_new_conversation():
    """Start a new conversation session, preserving agent's memory."""
    if not agent:
        raise HTTPException(status_code=503, detail="Agent not initialized")
    
    if not agent._initialized:
        raise HTTPException(status_code=503, detail="Agent still initializing")
    
    # Start new session (clears conversation but keeps memory)
    session_id = agent.conversation.start_new_session()
    
    return {
        "message": "New conversation session started. Agent memory preserved.",
        "session_id": session_id,
        "conversation_length": 0,
        "memory_preserved": True,
        "timestamp": asyncio.get_event_loop().time()
    }


@app.get("/knowledge/stats")
async def get_knowledge_stats():
    """Get statistics about the knowledge base (RAG)."""
    if not agent:
        raise HTTPException(status_code=503, detail="Agent not initialized")
    
    if not agent._initialized:
        raise HTTPException(status_code=503, detail="Agent still initializing")
    
    if not agent.rag_system:
        return {
            "enabled": False,
            "document_count": 0,
            "chunk_count": 0,
            "embedding_model": None
        }
    
    try:
        # Get collection info from ChromaDB
        collection = agent.rag_system.vector_store.collection
        count = collection.count()
        
        return {
            "enabled": True,
            "chunk_count": count,
            "embedding_model": agent.config.rag.embedding_model,
            "vector_db_path": agent.config.rag.vector_db_path,
            "chunk_size": agent.config.rag.chunk_size,
            "similarity_threshold": agent.config.rag.similarity_threshold
        }
    except Exception as e:
        return {
            "enabled": True,
            "error": str(e),
            "chunk_count": 0
        }


@app.get("/knowledge/search")
async def search_knowledge(query: str, limit: int = 10):
    """Search the knowledge base."""
    if not agent:
        raise HTTPException(status_code=503, detail="Agent not initialized")
    
    if not agent._initialized:
        raise HTTPException(status_code=503, detail="Agent still initializing")
    
    if not agent.rag_system:
        raise HTTPException(status_code=400, detail="RAG system not enabled")
    
    try:
        results = await agent.search_knowledge(query, k=limit)
        return {
            "query": query,
            "results": results,
            "result_count": len(results)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Search failed: {str(e)}")


@app.get("/memory/stats")
async def get_memory_stats():
    """Get memory statistics."""
    if not agent:
        raise HTTPException(status_code=503, detail="Agent not initialized")
    
    if not agent._initialized:
        raise HTTPException(status_code=503, detail="Agent still initializing")
    
    return {
        "conversation_length": len(agent.conversation.messages),
        "max_conversation_length": agent.conversation.max_length,
        "estimated_tokens": agent.conversation.get_context_length(),
        "persistence_enabled": agent.conversation.enable_persistence,
        "system_prompt_set": agent.conversation.system_prompt is not None
    }


def main():
    """Run the API server."""
    print("ü§ñ LLM Agent API - http://localhost:8001")
    print("üìñ Docs: http://localhost:8001/docs\n")
    
    # Start input listener in a separate thread
    input_thread = threading.Thread(target=listen_for_restart, daemon=True)
    input_thread.start()
    
    uvicorn.run(
        "api_server:app",
        host="0.0.0.0",
        port=8001,
        log_level="warning",  # Reduce noise
        reload=True,
        reload_dirs=["./src", "."]
    )


if __name__ == "__main__":
    main()
